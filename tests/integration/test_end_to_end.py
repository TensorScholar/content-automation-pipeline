"""
End-to-End Integration Tests

Validates complete system integration across all architectural layers:
- Project context resolution (3-layer hierarchy)
- Keyword research with semantic clustering
- Content planning and outline synthesis
- Article generation with quality validation
- Multi-channel distribution
- Observability and metrics collection

Testing Philosophy: Production scenario simulation with chaos engineering
Theoretical Foundation: Contract testing + temporal logic verification
"""

import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List
from unittest.mock import AsyncMock, Mock, patch
from uuid import UUID, uuid4

import pytest
import pytest_asyncio

from core.exceptions import TokenBudgetExceededError, WorkflowError
from core.models import Project
from execution.content_generator import ContentGenerator
from execution.content_planner import ContentPlanner
from execution.keyword_researcher import KeywordResearcher
from infrastructure.llm_client import LLMClient
from infrastructure.monitoring import MetricsCollector
from intelligence.context_synthesizer import ContextSynthesizer
from intelligence.decision_engine import DecisionEngine
from intelligence.semantic_analyzer import SemanticAnalyzer
from knowledge.article_repository import ArticleRepository
from knowledge.project_repository import ProjectRepository
from knowledge.rulebook_manager import RulebookManager
from knowledge.website_analyzer import WebsiteAnalyzer
from optimization.cache_manager import CacheManager
from optimization.model_router import ModelRouter
from optimization.prompt_compressor import PromptCompressor
from optimization.token_budget_manager import TokenBudgetManager
from orchestration.content_agent import ContentAgent, ContentAgentConfig, WorkflowState


# ============================================================================
# INTEGRATION TEST FIXTURES
# ============================================================================


@pytest_asyncio.fixture
async def integrated_system(clean_db, redis):
    """
    Fully integrated system with real components (mocked LLM only).

    Constructs complete dependency graph for end-to-end testing.
    Uses actual implementations except for expensive external calls.
    """
    # Infrastructure
    db = clean_db
    metrics = MetricsCollector()

    # Mock LLM client to avoid API costs
    llm = AsyncMock(spec=LLMClient)

    # Configure realistic LLM responses
    def mock_complete(**kwargs):
        prompt = kwargs.get("prompt", "")
        max_tokens = kwargs.get("max_tokens", 1000)

        # Generate realistic content based on prompt
        if "keyword" in prompt.lower():
            content = json.dumps(
                {
                    "keywords": [
                        {"phrase": "test keyword 1", "volume": 5000, "difficulty": 0.6},
                        {"phrase": "test keyword 2", "volume": 3000, "difficulty": 0.4},
                        {"phrase": "test keyword 3", "volume": 1500, "difficulty": 0.3},
                    ]
                }
            )
        elif "outline" in prompt.lower() or "plan" in prompt.lower():
            content = json.dumps(
                {
                    "title": "Advanced Guide to Test-Driven Content Generation",
                    "meta_description": "Comprehensive guide exploring modern approaches to automated content creation with quality assurance.",
                    "sections": [
                        {"heading": "Introduction to Automated Content", "words": 300},
                        {"heading": "Understanding NLP-Driven Generation", "words": 400},
                        {"heading": "Quality Validation Strategies", "words": 350},
                        {"heading": "Economic Optimization Techniques", "words": 300},
                        {"heading": "Conclusion and Future Directions", "words": 250},
                    ],
                }
            )
        else:
            # Article section content
            word_count = min(max_tokens // 2, 400)
            content = " ".join(
                [
                    f"This is generated content for testing purposes. "
                    f"The system demonstrates advanced NLP capabilities and intelligent content synthesis. "
                    f"Integration testing validates end-to-end workflow execution with realistic scenarios."
                ]
                * (word_count // 30)
            )

        return Mock(
            content=content,
            usage=Mock(
                prompt_tokens=len(prompt.split()) * 1.3,
                completion_tokens=len(content.split()) * 1.3,
            ),
            cost=0.015,
        )

    llm.complete = AsyncMock(side_effect=mock_complete)

    # Intelligence layer
    semantic_analyzer = SemanticAnalyzer()

    # Knowledge layer
    projects = ProjectRepository(db)

    # Mock the create method to return the project with an ID
    original_create = projects.create

    async def mock_create(project: Project) -> Project:
        if project.id is None:
            project.id = uuid4()
        return project

    projects.create = AsyncMock(side_effect=mock_create)

    rulebook_mgr = RulebookManager(db.session(), semantic_analyzer)

    # Create required dependencies for WebsiteAnalyzer
    from config.settings import get_settings
    from knowledge.pattern_extractor import PatternExtractor

    pattern_extractor = PatternExtractor(semantic_analyzer)
    scraping_settings = get_settings().scraping
    website_analyzer = WebsiteAnalyzer(pattern_extractor, projects, scraping_settings)

    # Continue with intelligence layer
    from intelligence.best_practices_kb import BestPracticesKB

    best_practices = BestPracticesKB()
    decision_engine = DecisionEngine(db.session(), rulebook_mgr, best_practices)
    cache = CacheManager(redis)
    context_synthesizer = ContextSynthesizer()

    # Optimization layer
    from optimization.token_budget_manager import BudgetConfig

    budget_config = BudgetConfig(daily_token_limit=10000, daily_cost_limit=50.0)
    budget_manager = TokenBudgetManager(budget_config)
    model_router = ModelRouter(budget_manager=budget_manager)
    prompt_compressor = PromptCompressor()

    # Create mock ArticleRepository
    article_repo = AsyncMock(spec=ArticleRepository)

    # Execution layer
    keyword_researcher = KeywordResearcher()
    content_planner = ContentPlanner(
        decision_engine=decision_engine,
        context_synthesizer=context_synthesizer,
        model_router=model_router,
        llm_client=llm,
        article_repository=article_repo,
        metrics_collector=metrics,
    )
    content_generator = ContentGenerator(
        model_router=model_router,
        llm_client=llm,
        context_synthesizer=context_synthesizer,
        semantic_analyzer=semantic_analyzer,
        token_budget_manager=budget_manager,
        article_repository=article_repo,
        metrics_collector=metrics,
    )

    # Orchestration
    agent = ContentAgent(
        database_manager=db,
        rulebook_manager=rulebook_mgr,
        website_analyzer=website_analyzer,
        decision_engine=decision_engine,
        context_synthesizer=context_synthesizer,
        keyword_researcher=keyword_researcher,
        content_planner=content_planner,
        content_generator=content_generator,
        budget_manager=budget_manager,
        metrics_collector=metrics,
        config=ContentAgentConfig(enable_auto_distribution=False, require_manual_approval=False),
    )

    return {
        "agent": agent,
        "projects": projects,
        "rulebook_mgr": rulebook_mgr,
        "db": db,
        "redis": redis,
        "metrics": metrics,
        "llm": llm,
    }


@pytest_asyncio.fixture
async def sample_project_with_rulebook(integrated_system):
    """Create fully configured project for testing."""
    db = integrated_system["db"]
    projects = integrated_system["projects"]
    rulebook_mgr = integrated_system["rulebook_mgr"]

    # Create project
    from core.models import Project

    project = Project(
        name="Integration Test Project",
        domain="https://integration-test.com",
        telegram_channel="@integration_test",
    )
    project = await projects.create(project)

    # Add rulebook
    rulebook_content = """
    # Content Guidelines
    
    ## Tone
    - Use professional yet accessible language
    - Target audience: technical professionals
    - Maintain authoritative voice without condescension
    
    ## Structure
    - Begin with compelling hook
    - Use clear hierarchical headings
    - Include practical examples
    - Conclude with actionable takeaways
    
    ## Quality Standards
    - Target Flesch-Kincaid grade 10-12
    - Keyword density: 0.5-2%
    - Article length: 1500-2500 words
    - Include internal links where relevant
    """

    # Mock create_rulebook to avoid database calls
    from datetime import datetime

    from core.models import Rule, Rulebook, RuleType

    mock_rulebook = Rulebook(
        id=uuid4(),
        project_id=project.id,
        raw_content=rulebook_content,
        version=1,
        created_at=datetime.now(),
        updated_at=datetime.now(),
        rules=[],
    )
    rulebook_mgr.create_rulebook = AsyncMock(return_value=mock_rulebook)

    return project


# ============================================================================
# HAPPY PATH INTEGRATION TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_complete_content_generation_workflow(
    integrated_system, sample_project_with_rulebook, mocker
):
    """
    Test complete workflow: topic â†’ published article.

    Validates all stages execute correctly with proper data flow.
    This is the golden path scenario.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    # Create mock ContentPlan
    from datetime import datetime, timezone

    from core.enums import KeywordIntent, SectionIntent
    from core.models import ContentPlan, Keyword, Outline, Section

    mock_outline = Outline(
        title="Advanced NLP Techniques for Content Automation",
        sections=[
            Section(heading="Introduction", intent=SectionIntent.INTRODUCE, estimated_words=300),
            Section(heading="Main Content", intent=SectionIntent.EXPLAIN, estimated_words=400),
            Section(heading="Conclusion", intent=SectionIntent.CONCLUDE, estimated_words=300),
        ],
        meta_description="Comprehensive guide to advanced natural language processing techniques for automating content generation workflows",
    )
    mock_plan = ContentPlan(
        project_id=project.id,
        topic="Advanced NLP Techniques for Content Automation",
        primary_keywords=[
            Keyword(phrase="NLP", intent=KeywordIntent.INFORMATIONAL),
            Keyword(phrase="automation", intent=KeywordIntent.INFORMATIONAL),
            Keyword(phrase="techniques", intent=KeywordIntent.INFORMATIONAL),
        ],
        secondary_keywords=[
            Keyword(phrase="AI", intent=KeywordIntent.INFORMATIONAL),
            Keyword(phrase="content", intent=KeywordIntent.INFORMATIONAL),
        ],
        outline=mock_outline,
        target_word_count=1500,
        created_at=datetime.now(timezone.utc),
    )

    # Create mock GeneratedArticle
    from core.models import GeneratedArticle, QualityMetrics

    mock_article = GeneratedArticle(
        project_id=project.id,
        content_plan_id=mock_plan.id,
        title="Advanced NLP Techniques for Content Automation",
        content="<h1>Advanced NLP Techniques</h1><p>This is comprehensive content about NLP techniques.</p>"
        * 50,
        meta_description="Comprehensive guide to advanced natural language processing techniques for automating content generation workflows",
        quality_metrics=QualityMetrics(
            readability_score=75.0,
            seo_score=80.0,
            engagement_score=70.0,
            factual_accuracy=95.0,
            coherence_score=85.0,
            word_count=1500,
            lexical_diversity=0.75,
            avg_sentence_length=15.0,
            paragraph_count=3,
        ),
        total_tokens_used=2000,
        total_cost_usd=0.05,
        generation_time_seconds=30.0,
    )

    # Mock the internal methods using mocker fixture
    mock_create_plan = mocker.patch.object(
        agent.content_planner, "create_content_plan", return_value=mock_plan
    )

    mock_generate_article = mocker.patch.object(
        agent.content_generator, "generate_article", return_value=mock_article
    )

    # Mock keyword research and project context loading
    mock_keywords = {"primary": [], "secondary": [], "long_tail": []}
    mocker.patch.object(agent, "_conduct_keyword_research", return_value=mock_keywords)

    mock_context = {
        "project": project,
        "rulebook": None,
        "inferred_patterns": None,
        "decision_strategy": "explicit_rules",
    }
    mocker.patch.object(agent, "_load_project_context", return_value=mock_context)

    # Mock validation to pass
    mocker.patch.object(
        agent, "_validate_article_quality", return_value={"passed": True, "issues": []}
    )

    # Execute complete workflow
    article = await agent.create_content(
        project_id=project.id,
        topic="Advanced NLP Techniques for Content Automation",
        priority="high",
    )

    # Validate article creation
    assert article is not None
    assert article.id is not None
    assert article.project_id == project.id
    assert article.title is not None

    # Validate that mocked methods were called
    mock_create_plan.assert_called_once()
    mock_generate_article.assert_called_once()


@pytest.mark.integration
@pytest.mark.asyncio
async def test_workflow_with_new_project_no_context(integrated_system, clean_db):
    """
    Test workflow with brand new project (no rulebook, no patterns).

    Should fall back to best practices and still generate quality content.
    """
    agent = integrated_system["agent"]
    projects = integrated_system["projects"]

    # Create minimal project
    from core.models import Project

    project = Project(name="New Project", domain=None, telegram_channel=None)
    project = await projects.create(project)

    # Generate content with no context
    article = await agent.create_content(
        project_id=project.id, topic="Getting Started with Content Automation", priority="medium"
    )

    # Should still succeed
    assert article is not None
    assert article.word_count > 500
    assert article.readability_score > 0

    # Workflow should use best practices
    workflow_status = await agent.get_workflow_status()
    events = workflow_status["events"]

    # Should have context loading event indicating Layer 3
    context_events = [e for e in events if e["state"] == WorkflowState.CONTEXT_LOADING]
    assert len(context_events) > 0


@pytest.mark.integration
@pytest.mark.asyncio
async def test_workflow_with_custom_instructions(integrated_system, sample_project_with_rulebook):
    """
    Test workflow with custom generation instructions.

    Custom instructions should override defaults while respecting rulebook.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    custom_instructions = """
    Focus heavily on practical implementation examples.
    Include code snippets where appropriate.
    Target advanced audience with deep technical knowledge.
    """

    article = await agent.create_content(
        project_id=project.id,
        topic="Building Production-Grade NLP Pipelines",
        priority="high",
        custom_instructions=custom_instructions,
    )

    assert article is not None
    assert article.word_count > 1000

    # Custom instructions should be reflected in metadata
    workflow_status = await agent.get_workflow_status()
    planning_events = [
        e for e in workflow_status["events"] if e["state"] == WorkflowState.CONTENT_PLANNING
    ]
    assert len(planning_events) > 0


# ============================================================================
# DECISION HIERARCHY INTEGRATION TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_layer1_explicit_rules_precedence(integrated_system, sample_project_with_rulebook):
    """
    Test that explicit rulebook rules take precedence.

    Validates Layer 1 (explicit) > Layer 2 (inferred) > Layer 3 (best practices).
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    article = await agent.create_content(
        project_id=project.id, topic="Technical Documentation Best Practices", priority="high"
    )

    # Workflow should have used explicit rules
    workflow_status = await agent.get_workflow_status()
    context_event = next(
        e for e in workflow_status["events"] if e["state"] == WorkflowState.CONTEXT_LOADING
    )

    # Should indicate explicit rules strategy
    assert context_event["metadata"]["strategy"] == "explicit_rules"

    # Article should reflect rulebook guidance
    # (Professional tone, grade 10-12 readability)
    assert 50 <= article.readability_score <= 80  # Grade 10-12 range


@pytest.mark.integration
@pytest.mark.asyncio
async def test_layer2_inferred_patterns_fallback(integrated_system, clean_db):
    """
    Test Layer 2 fallback when no explicit rules exist.

    Creates project with inferred patterns but no rulebook.
    """
    agent = integrated_system["agent"]
    projects = integrated_system["projects"]
    db = clean_db

    # Create project
    from core.models import Project

    project = Project(name="Pattern Project", domain="https://pattern.com")
    project = await projects.create(project)

    # Add inferred patterns
    import numpy as np

    pattern_id = uuid4()
    tone_embedding = np.random.rand(384).astype(np.float32)

    await db.execute(
        """
        INSERT INTO inferred_patterns 
        (id, project_id, avg_sentence_length, sentence_length_std,
         lexical_diversity, readability_score, tone_embedding, confidence, sample_size)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
        """,
        pattern_id,
        project.id,
        18.5,
        2.3,
        0.72,
        65.0,
        tone_embedding.tolist(),
        0.85,
        15,
    )

    # Generate content
    article = await agent.create_content(
        project_id=project.id, topic="Content Strategy Analysis", priority="medium"
    )

    assert article is not None

    # Should have used inferred patterns
    workflow_status = await agent.get_workflow_status()
    context_event = next(
        e for e in workflow_status["events"] if e["state"] == WorkflowState.CONTEXT_LOADING
    )

    assert context_event["metadata"]["strategy"] == "inferred_patterns"


# ============================================================================
# QUALITY VALIDATION INTEGRATION TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_quality_validation_rejects_poor_content(
    integrated_system, sample_project_with_rulebook
):
    """
    Test quality validation with intentionally poor content.

    Simulates generation failure and validates retry mechanism.
    """
    agent = integrated_system["agent"]
    llm = integrated_system["llm"]
    project = sample_project_with_rulebook

    # Mock LLM to return poor quality content first time
    call_count = 0

    def poor_then_good_content(**kwargs):
        nonlocal call_count
        call_count += 1

        if call_count <= 3:
            # First few calls: very short, low quality
            content = "Short bad content." * 10
        else:
            # Subsequent calls: good quality
            content = (
                "This is high quality generated content with proper length and structure. " * 50
            )

        return Mock(
            content=content,
            usage=Mock(prompt_tokens=100, completion_tokens=len(content.split())),
            cost=0.015,
        )

    llm.complete = AsyncMock(side_effect=poor_then_good_content)

    # Should retry and eventually succeed
    article = await agent.create_content(
        project_id=project.id, topic="Quality Content Generation", priority="high"
    )

    # Should have good quality despite initial failures
    assert article.word_count > 500
    assert call_count > 3  # Proves retry happened


@pytest.mark.integration
@pytest.mark.asyncio
async def test_keyword_density_validation(integrated_system, sample_project_with_rulebook):
    """
    Test keyword density validation.

    Ensures generated content has appropriate keyword integration.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    article = await agent.create_content(
        project_id=project.id, topic="SEO Optimization Strategies", priority="high"
    )

    # Validate keyword density is within acceptable range
    for keyword, density in article.keyword_density.items():
        assert 0.005 <= density <= 0.025, f"Keyword '{keyword}' density {density} out of range"


# ============================================================================
# ECONOMIC OPTIMIZATION INTEGRATION TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_cost_stays_under_budget(integrated_system, sample_project_with_rulebook):
    """
    Test that generation cost stays within economic targets.

    Target: $0.08-0.30 per article
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    # Generate multiple articles to test consistency
    articles = []
    for i in range(3):
        article = await agent.create_content(
            project_id=project.id, topic=f"Test Topic {i}: Content Automation", priority="high"
        )
        articles.append(article)

    # Validate costs
    for article in articles:
        assert article.total_cost < 0.50, f"Cost ${article.total_cost:.2f} exceeds target"
        assert article.total_cost > 0.01, "Cost suspiciously low"

        # Cost per word should be reasonable
        cost_per_word = article.total_cost / article.word_count
        assert cost_per_word < 0.001, f"Cost per word ${cost_per_word:.4f} too high"


@pytest.mark.integration
@pytest.mark.asyncio
async def test_cache_hit_reduces_cost(integrated_system, sample_project_with_rulebook):
    """
    Test that cache hits reduce generation cost.

    Generate same topic twice; second should be cheaper due to caching.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook
    topic = "Cache Test: Identical Topic Generation"

    # First generation
    article1 = await agent.create_content(project_id=project.id, topic=topic, priority="high")

    # Second generation (should hit cache)
    article2 = await agent.create_content(project_id=project.id, topic=topic, priority="high")

    # Second should be faster and/or cheaper
    # (In practice, full cache hit would skip generation entirely,
    #  but partial hits still provide benefit)
    assert article2.generation_time <= article1.generation_time * 1.2  # Within 20%


@pytest.mark.integration
@pytest.mark.asyncio
async def test_token_budget_enforcement(integrated_system, sample_project_with_rulebook):
    """
    Test token budget hard limits are enforced.

    Attempts generation with very low budget to trigger limit.
    """
    agent = integrated_system["agent"]
    budget_manager = integrated_system["agent"].budget_manager
    project = sample_project_with_rulebook

    # Set very low daily budget
    await budget_manager.set_daily_budget(1000)  # Only 1k tokens

    # Attempt generation (should fail or be heavily constrained)
    with pytest.raises((TokenBudgetExceededError, WorkflowError)):
        await agent.create_content(
            project_id=project.id,
            topic="Very Long Comprehensive Guide Requiring Many Tokens",
            priority="low",  # Low priority gets less budget
        )


# ============================================================================
# DISTRIBUTION INTEGRATION TESTS
# ============================================================================


# ============================================================================
# CONCURRENCY & PERFORMANCE TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.slow
@pytest.mark.asyncio
async def test_concurrent_article_generation(integrated_system, sample_project_with_rulebook):
    """
    Test concurrent article generation.

    Validates system handles parallel requests without interference.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    topics = [
        "Concurrent Generation Test 1",
        "Concurrent Generation Test 2",
        "Concurrent Generation Test 3",
        "Concurrent Generation Test 4",
        "Concurrent Generation Test 5",
    ]

    # Generate concurrently
    tasks = [
        agent.create_content(project_id=project.id, topic=topic, priority="medium")
        for topic in topics
    ]

    articles = await asyncio.gather(*tasks, return_exceptions=True)

    # All should succeed
    successful = [a for a in articles if not isinstance(a, Exception)]
    assert len(successful) == len(topics)

    # Each should be unique
    titles = [a.title for a in successful]
    assert len(set(titles)) == len(titles)  # All unique


@pytest.mark.integration
@pytest.mark.slow
@pytest.mark.asyncio
async def test_end_to_end_latency_under_3_minutes(
    integrated_system, sample_project_with_rulebook, benchmark_timer
):
    """
    Test end-to-end workflow completes within performance target.

    Target: < 3 minutes for standard article generation.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    with benchmark_timer() as timer:
        article = await agent.create_content(
            project_id=project.id, topic="Performance Test Article", priority="high"
        )

    # Should complete in reasonable time
    assert timer.elapsed < 180  # 3 minutes
    assert article is not None


# ============================================================================
# OBSERVABILITY & MONITORING TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_metrics_collection_throughout_workflow(
    integrated_system, sample_project_with_rulebook
):
    """
    Test that metrics are collected at all workflow stages.

    Validates observability infrastructure integration.
    """
    agent = integrated_system["agent"]
    metrics = integrated_system["metrics"]
    project = sample_project_with_rulebook

    # Clear any existing metrics
    await metrics.reset()

    article = await agent.create_content(
        project_id=project.id, topic="Metrics Collection Test", priority="high"
    )

    # Validate metrics were collected
    # TODO: Implement actual metrics retrieval and validation
    # For now, just ensure workflow completed
    assert article is not None


@pytest.mark.integration
@pytest.mark.asyncio
async def test_workflow_event_sourcing(integrated_system, sample_project_with_rulebook):
    """
    Test complete workflow event sourcing for audit trail.

    Validates all major workflow stages are recorded.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    article = await agent.create_content(
        project_id=project.id, topic="Event Sourcing Test", priority="high"
    )

    workflow_status = await agent.get_workflow_status()
    events = workflow_status["events"]

    # Validate required workflow stages present
    expected_states = [
        WorkflowState.CONTEXT_LOADING,
        WorkflowState.KEYWORD_RESEARCH,
        WorkflowState.CONTENT_PLANNING,
        WorkflowState.CONTENT_GENERATION,
        WorkflowState.QUALITY_VALIDATION,
        WorkflowState.COMPLETED,
    ]

    event_states = [e["state"] for e in events]

    for expected_state in expected_states:
        assert expected_state in event_states, f"Missing workflow state: {expected_state}"


# ============================================================================
# FAILURE & RECOVERY TESTS (Chaos Engineering)
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_database_connection_failure_recovery(
    integrated_system, sample_project_with_rulebook
):
    """
    Test recovery from transient database failures.

    Simulates database connection loss and validates retry logic.
    """
    agent = integrated_system["agent"]
    db = integrated_system["db"]
    project = sample_project_with_rulebook

    # Simulate intermittent database failure
    original_execute = db.execute
    call_count = 0

    async def failing_execute(*args, **kwargs):
        nonlocal call_count
        call_count += 1

        # Fail first 2 calls, then succeed
        if call_count <= 2:
            raise Exception("Database connection lost")

        return await original_execute(*args, **kwargs)

    db.execute = failing_execute

    # Should retry and succeed
    article = await agent.create_content(
        project_id=project.id, topic="Database Failure Recovery Test", priority="high"
    )

    assert article is not None
    assert call_count > 2  # Proves retry happened


@pytest.mark.integration
@pytest.mark.asyncio
async def test_llm_api_timeout_handling(integrated_system, sample_project_with_rulebook):
    """
    Test handling of LLM API timeouts.

    Validates timeout detection and retry logic.
    """
    agent = integrated_system["agent"]
    llm = integrated_system["llm"]
    project = sample_project_with_rulebook

    # Simulate timeout then success
    call_count = 0

    async def timeout_then_success(**kwargs):
        nonlocal call_count
        call_count += 1

        if call_count == 1:
            await asyncio.sleep(0.1)
            raise asyncio.TimeoutError("API timeout")

        # Success on retry
        return Mock(
            content="Successful content after timeout",
            usage=Mock(prompt_tokens=100, completion_tokens=200),
            cost=0.015,
        )

    llm.complete = AsyncMock(side_effect=timeout_then_success)

    # Should handle timeout and retry
    article = await agent.create_content(
        project_id=project.id, topic="Timeout Handling Test", priority="high"
    )

    assert article is not None
    assert call_count > 1  # Retry occurred


@pytest.mark.integration
@pytest.mark.asyncio
async def test_partial_workflow_failure_cleanup(integrated_system, sample_project_with_rulebook):
    """
    Test cleanup on mid-workflow failures.

    Validates state management and resource cleanup.
    """
    agent = integrated_system["agent"]
    llm = integrated_system["llm"]
    project = sample_project_with_rulebook

    # Cause failure during content generation
    async def always_fail(**kwargs):
        raise Exception("Intentional generation failure")

    llm.complete = AsyncMock(side_effect=always_fail)

    # Should fail gracefully
    with pytest.raises(WorkflowError):
        await agent.create_content(
            project_id=project.id, topic="Failure Cleanup Test", priority="high"
        )

    # Workflow should have failed state
    workflow_status = await agent.get_workflow_status()
    assert workflow_status["state"] == WorkflowState.FAILED


# ============================================================================
# CONTRACT TESTING
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_article_schema_contract(integrated_system, sample_project_with_rulebook):
    """
    Test generated article adheres to schema contract.

    Validates all required fields present with correct types.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    article = await agent.create_content(
        project_id=project.id, topic="Schema Contract Test", priority="high"
    )

    # Validate schema contract
    assert isinstance(article.id, UUID)
    assert isinstance(article.project_id, UUID)
    assert isinstance(article.title, str) and len(article.title) > 0
    assert isinstance(article.content, str) and len(article.content) > 0
    assert isinstance(article.meta_description, str)
    assert isinstance(article.word_count, int) and article.word_count > 0
    assert isinstance(article.readability_score, (int, float))
    assert isinstance(article.keyword_density, dict)
    assert isinstance(article.total_tokens_used, int) and article.total_tokens_used > 0
    assert isinstance(article.total_cost, (int, float)) and article.total_cost > 0
    assert isinstance(article.generation_time, (int, float)) and article.generation_time > 0
    assert isinstance(article.created_at, datetime)
    # Validate optional fields

    if article.distributed_at is not None:
        assert isinstance(article.distributed_at, datetime)

    if article.distribution_channels is not None:
        assert isinstance(article.distribution_channels, list)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_workflow_state_machine_contract(integrated_system, sample_project_with_rulebook):
    """
    Test workflow state transitions follow valid state machine.
    Validates temporal logic: states must progress in valid order.
    """
    agent = integrated_system["agent"]
    project = sample_project_with_rulebook

    article = await agent.create_content(
        project_id=project.id, topic="State Machine Contract Test", priority="high"
    )

    workflow_status = await agent.get_workflow_status()
    events = workflow_status["events"]

    # Define valid state transitions
    valid_transitions = {
        WorkflowState.INITIALIZED: [WorkflowState.CONTEXT_LOADING],
        WorkflowState.CONTEXT_LOADING: [WorkflowState.KEYWORD_RESEARCH],
        WorkflowState.KEYWORD_RESEARCH: [WorkflowState.CONTENT_PLANNING],
        WorkflowState.CONTENT_PLANNING: [WorkflowState.CONTENT_GENERATION],
        WorkflowState.CONTENT_GENERATION: [WorkflowState.QUALITY_VALIDATION],
        WorkflowState.QUALITY_VALIDATION: [
            WorkflowState.DISTRIBUTION,
            WorkflowState.COMPLETED,
            WorkflowState.CONTENT_GENERATION,  # Retry allowed
        ],
        WorkflowState.DISTRIBUTION: [WorkflowState.COMPLETED],
    }

    # Validate state transitions
    states = [e["state"] for e in events]

    for i in range(len(states) - 1):
        current_state = states[i]
        next_state = states[i + 1]

        if current_state in valid_transitions:
            assert (
                next_state in valid_transitions[current_state]
            ), f"Invalid transition: {current_state} â†’ {next_state}"


# ============================================================================
# AUTHENTICATION & AUTHORIZATION INTEGRATION TESTS
# ============================================================================


@pytest.mark.integration
@pytest.mark.asyncio
async def test_full_auth_workflow_register_login_create_project(api_client):
    """
    Test complete authentication workflow: register -> login -> access protected endpoint.
    This test is refactored to use FastAPI dependency overrides for the UserService.
    """
    from datetime import datetime
    from typing import Optional
    from unittest.mock import AsyncMock
    from uuid import uuid4

    from api.main import app
    from api.routes.auth import get_user_service_dependency
    from core.models import UserCreate
    from security import User, UserInDB, decode_access_token, get_password_hash, verify_password
    from services.user_service import UserService

    # This test requires a running database, but we mock the service layer
    # to isolate the auth routes.

    mock_user_service = AsyncMock(spec=UserService)
    created_users_db = {}  # Mock in-memory DB

    async def mock_create_user(user_create: UserCreate) -> User:
        user_id = uuid4()
        hashed_password = get_password_hash(user_create.password)
        user_db = UserInDB(
            id=user_id,
            email=user_create.email,
            hashed_password=hashed_password,
            full_name=user_create.full_name,
            is_active=True,
            is_superuser=False,
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
        )
        created_users_db[user_db.email] = user_db

        # Return the public User model
        return User(
            id=user_id,
            email=user_db.email,
            full_name=user_db.full_name,
            is_active=user_db.is_active,
            is_superuser=user_db.is_superuser,
            created_at=user_db.created_at,
            updated_at=user_db.updated_at,
        )

    async def mock_authenticate_user(username, password) -> Optional[UserInDB]:
        user_db = created_users_db.get(username)
        if user_db and verify_password(password, user_db.hashed_password):
            return user_db
        return None

    async def mock_get_user_by_email(email) -> Optional[UserInDB]:
        return created_users_db.get(email)

    mock_user_service.create_user = mock_create_user
    mock_user_service.authenticate_user = mock_authenticate_user
    mock_user_service.get_user_by_email = mock_get_user_by_email

    # Override the dependency
    app.dependency_overrides[get_user_service_dependency] = lambda: mock_user_service

    try:
        # Step 1: Register a new user
        registration_data = {
            "email": "testuser@example.com",
            "password": "SecurePassword123!",
            "full_name": "Test User",
        }

        register_response = api_client.post("/auth/register", json=registration_data)
        assert register_response.status_code in [200, 201]  # 200 is default, 201 is also common
        user_data = register_response.json()
        assert user_data["email"] == "testuser@example.com"
        assert "hashed_password" not in user_data

        # Step 2: Login to get JWT token
        login_data = {"username": "testuser@example.com", "password": "SecurePassword123!"}

        token_response = api_client.post(
            "/auth/token",
            data=login_data,  # OAuth2PasswordRequestForm uses form data
        )
        assert token_response.status_code == 200
        token_data = token_response.json()

        assert "access_token" in token_data
        assert token_data["token_type"] == "bearer"
        access_token = token_data["access_token"]

        # Step 3: Verify can access user profile with token
        me_response = api_client.get(
            "/auth/me", headers={"Authorization": f"Bearer {access_token}"}
        )
        assert me_response.status_code == 200
        user_profile = me_response.json()

        assert user_profile["email"] == "testuser@example.com"
        assert user_profile["full_name"] == "Test User"
        assert user_profile["is_active"] is True

    finally:
        # Cleanup dependency overrides
        app.dependency_overrides = {}


@pytest.mark.integration
@pytest.mark.asyncio
async def test_protected_endpoint_without_token_returns_401(api_client):
    """
    Test that accessing protected endpoint without token returns 401 Unauthorized.

    Validates:
    - Authentication middleware is working
    - Proper error response for unauthenticated requests
    - Security headers are set correctly
    """
    # Attempt to create project without authentication
    project_data = {"name": "Unauthorized Project", "domain": "https://unauthorized.com"}

    response = api_client.post("/projects/", json=project_data)

    # Should return 401 Unauthorized
    assert response.status_code == 401
    error_data = response.json()

    assert "detail" in error_data
    assert "Not authenticated" in error_data["detail"]

    # Verify WWW-Authenticate header is present
    assert "WWW-Authenticate" in response.headers


@pytest.mark.integration
@pytest.mark.asyncio
async def test_protected_endpoint_with_invalid_token_returns_401(api_client):
    """
    Test that accessing protected endpoint with invalid token returns 401.

    Validates:
    - Token validation is working
    - Invalid tokens are rejected
    - Proper error messaging
    """
    # Attempt to access protected endpoint with invalid token
    invalid_token = "invalid.jwt.token.here"

    response = api_client.get("/auth/me", headers={"Authorization": f"Bearer {invalid_token}"})

    # Should return 401 Unauthorized
    assert response.status_code == 401
    error_data = response.json()

    assert "detail" in error_data
    assert "Could not validate credentials" in error_data["detail"]


@pytest.mark.integration
@pytest.mark.asyncio
async def test_expired_token_returns_401(api_client, clean_db):
    """
    Test that expired tokens are rejected.

    Validates:
    - Token expiration is enforced
    - Expired tokens cannot access protected resources
    """
    from datetime import timedelta

    from security import create_access_token

    # Create a token that's already expired
    expired_token = create_access_token(
        data={"sub": "test@example.com", "user_id": "123"},
        expires_delta=timedelta(seconds=-10),  # Expired 10 seconds ago
    )

    response = api_client.get("/auth/me", headers={"Authorization": f"Bearer {expired_token}"})

    # Should return 401 Unauthorized
    assert response.status_code == 401
    error_data = response.json()

    assert "detail" in error_data
    assert "Could not validate credentials" in error_data["detail"]


@pytest.mark.integration
@pytest.mark.asyncio
async def test_inactive_user_cannot_access_protected_endpoints(api_client, clean_db):
    """
    Test that inactive users cannot access protected endpoints.

    Validates:
    - User activation status is checked
    - Inactive users are denied access even with valid token
    """
    # Register user
    registration_data = {
        "email": "inactive@example.com",
        "password": "Password123!",
        "full_name": "Inactive User",
    }

    api_client.post("/auth/register", json=registration_data)

    # Login to get token
    login_data = {"username": "inactive@example.com", "password": "Password123!"}

    token_response = api_client.post(
        "/auth/token",
        data=login_data,
        headers={"Content-Type": "application/x-www-form-urlencoded"},
    )
    access_token = token_response.json()["access_token"]

    # TODO: Add logic to deactivate user in database
    # For now, this test documents the expected behavior

    # Attempt to access protected endpoint with valid token but inactive user
    # Should return 400 Bad Request with "Inactive user" message
    # (Implementation depends on your user deactivation logic)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_token_contains_correct_user_information(api_client, clean_db):
    """
    Test that JWT tokens contain correct user information.

    Validates:
    - Token payload includes user ID and email
    - Token can be decoded to retrieve user information
    - Scopes are correctly assigned
    """
    # Register and login
    registration_data = {
        "email": "tokentest@example.com",
        "password": "Password123!",
        "full_name": "Token Test User",
    }

    register_response = api_client.post("/auth/register", json=registration_data)
    user_id = register_response.json()["id"]

    login_data = {"username": "tokentest@example.com", "password": "Password123!"}

    token_response = api_client.post(
        "/auth/token",
        data=login_data,
        headers={"Content-Type": "application/x-www-form-urlencoded"},
    )

    access_token = token_response.json()["access_token"]

    # Decode token to verify payload
    from security import decode_access_token

    token_data = decode_access_token(access_token)

    assert token_data.username == "tokentest@example.com"
    assert token_data.user_id == str(user_id)
    assert token_data.scopes is not None
    assert len(token_data.scopes) > 0
    assert token_data.expires_at is not None


@pytest.mark.integration
@pytest.mark.asyncio
async def test_superuser_can_access_admin_endpoints(api_client, clean_db):
    """
    Test that superusers can access admin-only endpoints.

    Validates:
    - Superuser role is correctly assigned
    - Admin endpoints check superuser status
    - Regular users cannot access admin endpoints
    """
    # This test requires superuser creation logic in your system
    # For now, this documents the expected behavior

    # TODO: Implement once admin endpoints are defined
    pass
